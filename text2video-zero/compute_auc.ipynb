{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from statistics import mean, stdev\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "\n",
    "from tree_ring.inverse_stable_diffusion import InversableStableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "import open_clip\n",
    "from tree_ring.optim_utils import *\n",
    "from tree_ring.io_utils import *\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='diffusion watermark')\n",
    "# parser.add_argument('--run_name', default='test')\n",
    "parser.add_argument('--run_name', default='no_attack')\n",
    "parser.add_argument('--dataset', default='./Stable-Diffusion-Prompts')\n",
    "parser.add_argument('--start', default=0, type=int)\n",
    "# parser.add_argument('--end', default=10, type=int)\n",
    "parser.add_argument('--end', default=1000, type=int)\n",
    "parser.add_argument('--image_length', default=512, type=int)\n",
    "parser.add_argument('--model_id', default='stabilityai/stable-diffusion-2-1-base')\n",
    "parser.add_argument('--with_tracking', action='store_true')\n",
    "parser.add_argument('--num_images', default=1, type=int)\n",
    "parser.add_argument('--guidance_scale', default=7.5, type=float)\n",
    "parser.add_argument('--num_inference_steps', default=50, type=int)\n",
    "parser.add_argument('--test_num_inference_steps', default=None, type=int)\n",
    "parser.add_argument('--reference_model', default=None)\n",
    "parser.add_argument('--reference_model_pretrain', default=None)\n",
    "parser.add_argument('--max_num_log_image', default=100, type=int)\n",
    "parser.add_argument('--gen_seed', default=0, type=int)\n",
    "\n",
    "# watermark\n",
    "parser.add_argument('--w_seed', default=999999, type=int)\n",
    "# parser.add_argument('--w_channel', default=0, type=int)\n",
    "parser.add_argument('--w_channel', default=3, type=int)\n",
    "# parser.add_argument('--w_pattern', default='rand')\n",
    "parser.add_argument('--w_pattern', default='ring')\n",
    "parser.add_argument('--w_mask_shape', default='circle')\n",
    "parser.add_argument('--w_radius', default=10, type=int)\n",
    "parser.add_argument('--w_measurement', default='l1_complex')\n",
    "parser.add_argument('--w_injection', default='complex')\n",
    "parser.add_argument('--w_pattern_const', default=0, type=float)\n",
    "\n",
    "# for image distortion\n",
    "parser.add_argument('--r_degree', default=None, type=float)\n",
    "parser.add_argument('--jpeg_ratio', default=None, type=int)\n",
    "parser.add_argument('--crop_scale', default=None, type=float)\n",
    "parser.add_argument('--crop_ratio', default=None, type=float)\n",
    "parser.add_argument('--gaussian_blur_r', default=None, type=int)\n",
    "parser.add_argument('--gaussian_std', default=None, type=float)\n",
    "parser.add_argument('--brightness_factor', default=None, type=float)\n",
    "parser.add_argument('--orig_img_no_w_path',default='../datas/videos/text2video-zero/16frames_uniform/' ,type=str)\n",
    "parser.add_argument('--orig_img_w_path',default='../datas/videos/text2video-zero/16frames_uniform_w/', type=str)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "if args.test_num_inference_steps is None:\n",
    "    args.test_num_inference_steps = args.num_inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DPMSolverMultistepScheduler.from_pretrained(args.model_id, subfolder='scheduler')\n",
    "pipe = InversableStableDiffusionPipeline.from_pretrained(\n",
    "    args.model_id,\n",
    "    scheduler=scheduler,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision='fp16',\n",
    "    )\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_prompt = '' # assume at the detection time, the original prompt is unknown\n",
    "text_embeddings = pipe.get_text_embedding(tester_prompt)\n",
    "\n",
    "# ground-truth patch\n",
    "gt_patch = torch.load('./tree_ring/key.pt').to(dtype=torch.complex32)\n",
    "init_latents_w = pipe.get_random_latents()\n",
    "watermarking_mask = get_watermarking_mask(init_latents_w, args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_no_w = os.listdir(args.orig_img_no_w_path)\n",
    "folders_w = os.listdir(args.orig_img_w_path)\n",
    "for video in folders_no_w:\n",
    "    frames = os.listdir(os.path.join(args.orig_img_no_w_path,video))\n",
    "    for frame in frames:\n",
    "        frame_path_no_w = os.path.join(args.orig_img_no_w_path, video, frame)\n",
    "        frame_path_w = os.path.join(args.orig_img_w_path, video, frame)\n",
    "        \n",
    "        img_no_w = Image.open(frame_path_no_w)\n",
    "        img_w = Image.open(frame_path_w)\n",
    "        \n",
    "        img_no_w = transform_img(img_no_w).unsqueeze(0).to(text_embeddings.dtype).to(device)\n",
    "        img_w = transform_img(img_w).unsqueeze(0).to(text_embeddings.dtype).to(device)\n",
    "        \n",
    "        image_latents_no_w = pipe.get_image_latents(img_no_w, sample=False)\n",
    "        image_latents_w = pipe.get_image_latents(img_w, sample=False)\n",
    "        \n",
    "        reversed_latents_no_w = pipe.forward_diffusion(\n",
    "        latents=image_latents_no_w,\n",
    "        text_embeddings=text_embeddings,\n",
    "        guidance_scale=1,\n",
    "        num_inference_steps=args.test_num_inference_steps,\n",
    "        )\n",
    "        reversed_latents_w = pipe.forward_diffusion(\n",
    "        latents=image_latents_w,\n",
    "        text_embeddings=text_embeddings,\n",
    "        guidance_scale=1,\n",
    "        num_inference_steps=args.test_num_inference_steps,\n",
    "        )\n",
    "        \n",
    "        no_w_metric, w_metric = eval_watermark(reversed_latents_no_w, reversed_latents_w, watermarking_mask, gt_patch, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "integrate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
